{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Format the message string so that it shows the actual return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "# from preprocessing_helpers import convert_to_int\n",
    "\n",
    "# def test_on_string_with_one_comma():\n",
    "#     test_argument = \"2,081\"\n",
    "#     expected = 2081\n",
    "#     actual = convert_to_int(test_argument)\n",
    "#     # Format the string with the actual return value\n",
    "#     message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write the assert statement that checks if actual is equal to expected and prints the message message if they are not equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "# from preprocessing_helpers import convert_to_int\n",
    "\n",
    "# def test_on_string_with_one_comma():\n",
    "#     test_argument = \"2,081\"\n",
    "#     expected = 2081\n",
    "#     actual = convert_to_int(test_argument)\n",
    "#     # Format the string with the actual return value\n",
    "#     message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
    "#     # Write the assert statement which prints message on failure\n",
    "#     assert actual == expected, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pytest with command `!pytest test_convert_to_int.py` and interpret the report:\n",
    "```\n",
    ">       assert actual == expected, message\n",
    "E       AssertionError: convert_to_int('2,081') should return the int 2081, but it actually returned None\n",
    "E       assert None == 2081\n",
    "```\n",
    "\n",
    "- The test fails because convert_to_int(\"2,081\") returns None and not the integer 2081."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the assert statement to check if get_data_as_numpy_array() returns expected, when called on example_clean_data_file.txt with num_columns set to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pytest\n",
    "# from as_numpy import get_data_as_numpy_array\n",
    "\n",
    "# def test_on_clean_file():\n",
    "#   expected = np.array([[2081.0, 314942.0],\n",
    "#                        [1059.0, 186606.0],\n",
    "#   \t\t\t\t\t   [1148.0, 206186.0]\n",
    "#                        ]\n",
    "#                       )\n",
    "#   actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "#   message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "#   # Complete the assert statement\n",
    "#   assert actual == pytest.approx(expected), message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pytest.approx() function not only works for NumPy arrays containing floats, but also for lists and dictionaries containing floats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the expected number of rows of the training array using the formula int(0.75*n), where n is the number of rows in example_argument, and assign the variable expected_training_array_num_rows to this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_six_rows():\n",
    "    example_argument = np.array([\n",
    "        [2081.0, 314942.0], \n",
    "        [1059.0, 186606.0],\n",
    "        [1148.0, 206186.0], \n",
    "        [1506.0, 248419.0],\n",
    "        [1210.0, 214114.0], \n",
    "        [1697.0, 277794.0]])\n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = int(0.75*(example_argument.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the expected number of rows of the testing array using the formula n - int(0.75*n), where n is the number of rows in example_argument, and assign the variable expected_testing_array_num_rows to this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_six_rows():\n",
    "    example_argument = np.array([\n",
    "        [2081.0, 314942.0], \n",
    "        [1059.0, 186606.0],\n",
    "        [1148.0, 206186.0], \n",
    "        [1506.0, 248419.0],\n",
    "        [1210.0, 214114.0], \n",
    "        [1697.0, 277794.0]]\n",
    "                               )\n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = 4\n",
    "    # Fill in with testing array's expected number of rows\n",
    "    n = example_argument.shape[0]\n",
    "    expected_testing_array_num_rows = n - int(0.75*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a 2-tuple of NumPy arrays (training_set, testing_set). The training set contains int(0.75 * n) (approx. 75%) randomly selected rows of the argument array. The testing set contains the remaining rows.\n",
    "- Write an assert statement that checks if training array has expected_training_array_num_rows rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_six_rows():\n",
    "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
    "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
    "                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n",
    "                                )\n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = 4\n",
    "    # Fill in with testing array's expected number of rows\n",
    "    expected_testing_array_num_rows = 2\n",
    "    actual = split_into_training_and_testing_sets(example_argument)\n",
    "    # Write the assert statement checking training array's number of rows\n",
    "    assert actual[0].shape[0] == 4, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write an assert statement that checks if testing array has expected_testing_array_num_rows rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_six_rows():\n",
    "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
    "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
    "                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n",
    "                                )\n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = 4\n",
    "    # Fill in with testing array's expected number of rows\n",
    "    expected_testing_array_num_rows = 2\n",
    "    actual = split_into_training_and_testing_sets(example_argument)\n",
    "    # Write the assert statement checking training array's number of rows\n",
    "    assert actual[0].shape[0] == expected_training_array_num_rows, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
    "    # Write the assert statement checking testing array's number of rows\n",
    "    assert actual[1].shape[0] == expected_testing_array_num_rows, \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the with statement by filling in with a context manager that will silence the ValueError raised in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "\n",
    "# # Fill in with a context manager that will silence the ValueError\n",
    "# with pytest.raises(ValueError):\n",
    "#     raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the with statement with a context manager that raises Failed if no OSError is raised in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "\n",
    "# try:\n",
    "#     # Fill in with a context manager that raises Failed if no OSError is raised\n",
    "#     with pytest.raises(OSError):\n",
    "#         raise ValueError\n",
    "# except:\n",
    "#     print(\"pytest raised an exception because no OSError was raised in the context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extend the with statement so that any raised ValueError is stored in the variable exc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "\n",
    "# # Store the raised ValueError in the variable exc_info\n",
    "# with pytest.raises(ValueError) as exc_info:\n",
    "#     raise ValueError(\"Silence me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write an assert statement to check if the raised ValueError contains the message \"Silence me!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "\n",
    "# with pytest.raises(ValueError) as exc_info:\n",
    "#     raise ValueError(\"Silence me!\")\n",
    "# # Check if the raised ValueError contains the correct message\n",
    "# assert exc_info.match(\"Silence me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fill in with the correct context manager that checks if split_into_training_and_testing_sets() raises a ValueError when called on test_argument, which is a NumPy array with a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pytest\n",
    "# from train import split_into_training_and_testing_sets\n",
    "\n",
    "# def test_on_one_row():\n",
    "#     test_argument = np.array([[1382.0, 390167.0]])\n",
    "#     # Fill in with a context manager for checking ValueError\n",
    "#     with pytest.raises(ValueError):\n",
    "#       split_into_training_and_testing_sets(test_argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the with statement so that information about any raised ValueError will be stored in the variable exc_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pytest\n",
    "# from train import split_into_training_and_testing_sets\n",
    "\n",
    "# def test_on_one_row():\n",
    "#     test_argument = np.array([[1382.0, 390167.0]])\n",
    "#     # Store information about raised ValueError in exc_info\n",
    "#     with pytest.raises(ValueError) as exc_info:\n",
    "#       split_into_training_and_testing_sets(test_argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write an assert statement to check if the raised ValueError contains the correct message stored in the variable expected_error_msg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pytest\n",
    "# from train import split_into_training_and_testing_sets\n",
    "\n",
    "# def test_on_one_row():\n",
    "#     test_argument = np.array([[1382.0, 390167.0]])\n",
    "#     # Store information about raised ValueError in exc_info\n",
    "#     with pytest.raises(ValueError) as exc_info:\n",
    "#       split_into_training_and_testing_sets(test_argument)\n",
    "#     expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "#     # Check if the raised ValueError contains the correct message\n",
    "#     assert exc_info.match(expected_error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign actual to the return value of row_to_list() on the argument \"123\\n\", which is an instance of the boundary value (0, 0).\n",
    "- Complete the assert statement to check if row_to_list() indeed returns None for the instance \"123\\t4,567\\t89\\n\" of the boundary value (2, 0).\n",
    "- In the test test_on_one_tab_with_missing_value(), format the failure message with the actual return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n",
    "    # Assign actual to the return value for the argument \"123\\n\"\n",
    "    actual = row_to_list(\"123\\n\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_two_tabs_no_missing_value():    # (2, 0) boundary value\n",
    "    actual = row_to_list(\"123\\t4,567\\t89\\n\")\n",
    "    # Complete the assert statement\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n",
    "    actual = row_to_list(\"\\t4,567\\n\")\n",
    "    # Format the failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign the variable actual to the actual return value for \"\\n\".\n",
    "- Complete the assert statement for test_on_no_tab_with_missing_value(), making sure to format the failure message appropriately.\n",
    "- Assign the variable actual to the actual return value for \"123\\t\\t89\\n\".\n",
    "- Complete the assert statement for test_on_two_tabs_with_missing_value(), making sure to format the failure message appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "# from preprocessing_helpers import row_to_list\n",
    "\n",
    "# def test_on_no_tab_with_missing_value():    # (0, 1) case\n",
    "#     # Assign to the actual return value for the argument \"\\n\"\n",
    "#     actual = row_to_list(\"\\n\")\n",
    "#     # Write the assert statement with a failure message\n",
    "#     assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "# def test_on_two_tabs_with_missing_value():    # (2, 1) case\n",
    "#     # Assign to the actual return value for the argument \"123\\t\\t89\\n\"\n",
    "#     actual = row_to_list(\"123\\t\\t89\\n\")\n",
    "#     # Write the assert statement with a failure message\n",
    "#     assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many normal arguments is it recommended to test?\n",
    "- At least two or three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign the variable expected to the expected return value for the normal argument \"123\\t4,567\\n\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "# from preprocessing_helpers import row_to_list\n",
    "\n",
    "# def test_on_normal_argument_1():\n",
    "#     actual = row_to_list(\"123\\t4,567\\n\")\n",
    "#     # Fill in with the expected return value for the argument \"123\\t4,567\\n\"\n",
    "#     expected = ['123', '4,567']\n",
    "#     assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the correct assert statement for test_on_normal_argument_2(), making sure to format the failure message appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "# from preprocessing_helpers import row_to_list\n",
    "\n",
    "# def test_on_normal_argument_1():\n",
    "#     actual = row_to_list(\"123\\t4,567\\n\")\n",
    "#     # Fill in with the expected return value for the argument \"123\\t4,567\\n\"\n",
    "#     expected = [\"123\", \"4,567\"]\n",
    "#     assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
    "    \n",
    "# def test_on_normal_argument_2():\n",
    "#     actual = row_to_list(\"1,059\\t186,606\\n\")\n",
    "#     expected = [\"1,059\", \"186,606\"]\n",
    "#     # Write the assert statement along with a failure message\n",
    "#     assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the assert statement for test_with_no_comma() by inserting the correct boolean expression.\n",
    "- Complete the assert statement for test_with_one_comma() by inserting the correct boolean expression.\n",
    "- Complete the assert statement for test_with_two_commas() by inserting the correct boolean expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_no_comma():\n",
    "    actual = convert_to_int(\"756\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 756, \"Expected: 756, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_with_one_comma():\n",
    "    actual = convert_to_int(\"2,081\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 2081, \"Expected: 2081, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_with_two_commas():\n",
    "    actual = convert_to_int(\"1,034,891\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 1034891, \"Expected: 1034891, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Give a name to the test by using the standard name prefix that pytest expects followed by on_string_with_missing_comma.\n",
    "- Assign actual to the actual return value for the argument \"12,72,891\".\n",
    "- Complete the assert statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a name to the test for an argument with missing comma\n",
    "def test_on_string_with_missing_comma():\n",
    "    actual = convert_to_int(\"178100,301\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_string_with_incorrectly_placed_comma():\n",
    "    # Assign to the actual return value for the argument \"12,72,891\"\n",
    "    actual = convert_to_int(\"12,72,891\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_float_valued_string():\n",
    "    actual = convert_to_int(\"23,816.92\")\n",
    "    # Complete the assert statement\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tests for normal and special arguments have been written to a test module test_convert_to_int.py. Run it in the IPython console and read the test result report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pytest test_convert_to_int.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the if statement that checks if the i-th element of comma_separated_parts has length greater than 3.\n",
    "- Complete the if statement that checks if any entry other than the 0-th entry of comma_separated_parts has a length not equal to 3.\n",
    "- Fill in the except clause with a ValueError, which is raised when trying to convert float valued strings e.g. 23816.92 to an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(integer_string_with_commas):\n",
    "    comma_separated_parts = integer_string_with_commas.split(\",\")\n",
    "    for i in range(len(comma_separated_parts)):\n",
    "        # Write an if statement for checking missing commas\n",
    "        if len(comma_separated_parts[i]) > 3:\n",
    "            return None\n",
    "        # Write the if statement for incorrectly placed commas\n",
    "        if i != 0 and len(comma_separated_parts[i]) != 3:\n",
    "            return None\n",
    "    integer_string_without_commas = \"\".join(comma_separated_parts)\n",
    "    try:\n",
    "        return int(integer_string_without_commas)\n",
    "    # Fill in with a ValueError\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that you have implemented the convert_to_int() function, let's run the tests in the test module test_convert_to_int.py again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pytest test_convert_to_int.py"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
