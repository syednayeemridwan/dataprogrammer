{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add the correct decorator that would turn clean_data_file() into a fixture.\n",
    "- Pass an argument to the test test_on_clean_file() so that it uses the fixture.\n",
    "- Pass the clean data file path yielded by the fixture as the first argument to the function get_data_as_numpy_array()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add a decorator to make this function a fixture\n",
    "# @pytest.fixture\n",
    "# def clean_data_file():\n",
    "#     file_path = \"clean_data_file.txt\"\n",
    "#     with open(file_path, \"w\") as f:\n",
    "#         f.write(\"201\\t305671\\n7892\\t298140\\n501\\t738293\\n\")\n",
    "#     yield file_path\n",
    "#     os.remove(file_path)\n",
    "    \n",
    "# # Pass the correct argument so that the test can use the fixture\n",
    "# def test_on_clean_file(clean_data_file):\n",
    "#     expected = np.array([[201.0, 305671.0], [7892.0, 298140.0], [501.0, 738293.0]])\n",
    "#     # Pass the clean data file path yielded by the fixture as the first argument\n",
    "#     actual = get_data_as_numpy_array(clean_data_file, 2)\n",
    "#     assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the setup, assign the variable file_path to the correct string.\n",
    "- After the setup, yield the variable file_path so that the test can use it.\n",
    "- In the teardown, remove the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @pytest.fixture\n",
    "# def empty_file():\n",
    "#     # Assign the file path \"empty.txt\" to the variable\n",
    "#     file_path = \"empty.txt\"\n",
    "#     open(file_path, \"w\").close()\n",
    "#     # Yield the variable file_path\n",
    "#     yield file_path\n",
    "#     # Remove the file in the teardown\n",
    "#     os.remove(file_path)\n",
    "    \n",
    "# def test_on_empty_file(self, empty_file):\n",
    "#     expected = np.empty((0, 2))\n",
    "#     actual = get_data_as_numpy_array(empty_file, 2)\n",
    "#     assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add the correct argument to the fixture empty_file() so that it chains with the built-in fixture tmpdir.\n",
    "- Use the appropriate method to create an empty file \"empty.txt\" inside the temporary directory created by tmpdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "\n",
    "# @pytest.fixture\n",
    "# # Add the correct argument so that this fixture can chain with the tmpdir fixture\n",
    "# def empty_file(tmpdir):\n",
    "#     # Use the appropriate method to create an empty file in the temporary directory\n",
    "#     file_path = tmpdir.join(\"empty.txt\")\n",
    "#     open(file_path, \"w\").close()\n",
    "#     yield file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what order will the setup and teardown of empty_file() and tmpdir be executed?\n",
    "- setup of tmpdir  \n",
    "- setup of empty_file()  \n",
    "- teardown of empty_file()\n",
    "- teardown of tmpdir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function convert_to_int_bug_free() which takes one argument called comma_separated_integer_string.\n",
    "- Assign return_values to the dictionary holding the correct return values in the context of the raw data file used in the test.\n",
    "- Return the correct return value by looking up the dictionary return_values for the key comma_separated_integer_string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function convert_to_int_bug_free\n",
    "def convert_to_int_bug_free(comma_separated_integer_string):\n",
    "    # Assign to the dictionary holding the correct return values \n",
    "    return_values = {\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}\n",
    "    # Return the correct result using the dictionary return_values\n",
    "    return return_values[comma_separated_integer_string]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the test test_on_raw_data(), add the correct argument that enables the use of the mocking fixture.\n",
    "- Replace the dependency \"data.preprocessing_helpers.convert_to_int\" with the bug-free version convert_to_int_bug_free() by using the correct method and side effect.\n",
    "- Use the correct attribute which returns the list of calls to the mock, and check if the mock was called with this sequence of arguments: \"1,801\", \"201,411\", \"2,002\", \"333,209\", \"1990\", \"782,911\", \"1,285\", \"389129\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add the correct argument to use the mocking fixture in this test\n",
    "# def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "#     raw_path, clean_path = raw_and_clean_data_file\n",
    "#     # Replace the dependency with the bug-free mock\n",
    "#     convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
    "#                                        side_effect=convert_to_int_bug_free)\n",
    "#     preprocess(raw_path, clean_path)\n",
    "#     # Check if preprocess() called the dependency correctly\n",
    "#     assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
    "#     with open(clean_path, \"r\") as f:\n",
    "#         lines = f.readlines()\n",
    "#     first_line = lines[0]\n",
    "#     assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "#     second_line = lines[1]\n",
    "#     assert second_line == \"2002\\\\t333209\\\\n\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign the variable test_argument to a NumPy array holding the perfectly linear testing data printed out in the IPython console.\n",
    "- Assign the variable expected to the expected value of  in the special case of a perfect fit.\n",
    "- Fill in with the model's slope and intercept that matches the testing set.\n",
    "- Remembering that actual is a float, complete the assert statement to check if actual returned by model_test() is equal to the expected return value expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pytest\n",
    "# from models.train import model_test\n",
    "\n",
    "# def test_on_perfect_fit():\n",
    "#     # Assign to a NumPy array containing a linear testing set\n",
    "#     test_argument = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "#     # Fill in with the expected value of r^2 in the case of perfect fit\n",
    "#     expected = 1.0\n",
    "#     # Fill in with the slope and intercept of the model\n",
    "#     actual = model_test(test_argument, slope=2, intercept=1)\n",
    "#     # Complete the assert statement\n",
    "#     assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign test_argument to a  NumPy array holding the circular testing data shown in the plot, starting with (1.0, 0.0) and moving anticlockwise.\n",
    "- Fill in with the slope and intercept of the straight line shown in the plot.\n",
    "- Remembering that model_test() returns a float, complete the assert statement to check if model_test() returns the expected value of  in this special case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_circular_data(self):\n",
    "    theta = pi/4.0\n",
    "    # Assign to a NumPy array holding the circular testing data\n",
    "    test_argument = np.array([[1.0, 0.0], [cos(theta), sin(theta)],\n",
    "                              [0.0, 1.0],\n",
    "                              [cos(3 * theta), sin(3 * theta)],\n",
    "                              [-1.0, 0.0],\n",
    "                              [cos(5 * theta), sin(5 * theta)],\n",
    "                              [0.0, -1.0],\n",
    "                              [cos(7 * theta), sin(7 * theta)]]\n",
    "                             )\n",
    "    # Fill in with the slope and intercept of the straight line\n",
    "    actual = model_test(test_argument, slope=0.0, intercept=0.0)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests test_on_perfect_fit() and test_on_circular_data() that you wrote in the last two exercises has been written to the test class TestModelTest in the test module models/test_train.py. Run the test class in the IPython console. \n",
    "- `!pytest models/test_train.py::TestModelTest::test_on_perfect_fit`\n",
    "- `!pytest models/test_train.py::TestModelTest::test_on_circular_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add the correct pytest marker that helps in generating baselines and comparing images.\n",
    "- Return the matplotlib figure returned by the function under test.\n",
    "- The test that you wrote was written to a test class TestGetPlotForBestFitLine in the test module visualization/test_plots.py. In the shell console, execute the command required to create the baseline image for this test only. The baseline folder should be in project/tests/visualization. Because it's a shell console and not an IPython one, you don't need to use the ! at the beginning of your command. Once you've ran your command and created your baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "# import numpy as np\n",
    "\n",
    "# from visualization.plots import get_plot_for_best_fit_line\n",
    "\n",
    "# class TestGetPlotForBestFitLine(object):\n",
    "#     # Add the pytest marker which generates baselines and compares images\n",
    "#     @pytest.mark.mpl_image_compare\n",
    "#     def test_plot_for_almost_linear_data(self):\n",
    "#         slope = 5.0\n",
    "#         intercept = -2.0\n",
    "#         x_array = np.array([1.0, 2.0, 3.0])\n",
    "#         y_array = np.array([3.0, 8.0, 11.0])\n",
    "#         title = \"Test plot for almost linear data\"\n",
    "#         # Return the matplotlib figure returned by the function under test\n",
    "#         return get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pytest --mpl-generate-path /home/repl/workspace/project/tests/visualization/baseline -k \"test_plot_for_almost_linear_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the tests in this test class in the console. Because it's a shell console and not an IPython one, you don't need to use the ! at the beginning of your command. You should see two failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pytest -k \"TestGetPlotForBestFitLine\" --mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fill in the axis labels xlabel and ylabel so that they match the baseline plot (plot 1/2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title):\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.plot(x_array, y_array, \".\")\n",
    "#     ax.plot([0, np.max(x_array)], [intercept, slope * np.max(x_array) + intercept], \"-\")\n",
    "#     # Fill in with axis labels so that they match the baseline\n",
    "#     ax.set(ylabel=\"price (dollars)\", xlabel=\"area (square feet)\", title=title)\n",
    "#     return fig"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
