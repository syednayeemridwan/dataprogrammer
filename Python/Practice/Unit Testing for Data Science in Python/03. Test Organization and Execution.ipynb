{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "src/                                    # All application code lives here\n",
    "|-- visualization/                      # Package for visualization\n",
    "    |-- __init__.py\n",
    "    |-- plots.py                        # Module for plotting\n",
    "```\n",
    "\n",
    "In the package, there is a Python module plots.py, which contain functions related to plotting. These functions should be tested in a test module test_plots.py.\n",
    "\n",
    "According to pytest guidelines, where should you place this test module within the project structure?\n",
    "- `tests/visualization/test_plots.py.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Declare the test class for the function split_into_training_and_testing_sets(), making sure to give it a name that follows the standard naming convention.\n",
    "- Fill in the mandatory argument in the test test_on_one_row()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytest\n",
    "# import numpy as np\n",
    "\n",
    "# from models.train import split_into_training_and_testing_sets\n",
    "\n",
    "# # Declare the test class\n",
    "# class TestSplitIntoTrainingAndTestingSets(object):\n",
    "#     # Fill in with the correct mandatory argument\n",
    "#     def test_on_one_row(self):\n",
    "#         test_argument = np.array([[1382.0, 390167.0]])\n",
    "#         with pytest.raises(ValueError) as exc_info:\n",
    "#             split_into_training_and_testing_sets(test_argument)\n",
    "#         expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "#         assert exc_info.match(expected_error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the IPython console, what is the correct command for running all tests contained in the tests folder?\n",
    "- !pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that you simply want to answer the binary question \"Are all tests passing\" without wasting time and resources, what is the correct command to run all tests till the first failure is encountered?\n",
    "- !pytest -x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fill in with a float between 0 and 1 so that num_training is approximately 3/4 of the number of rows in data_array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_into_training_and_testing_sets(data_array):\n",
    "    dim = data_array.ndim\n",
    "    if dim != 2:\n",
    "        raise ValueError(\"Argument data_array must be two dimensional. Got {0} dimensional array instead!\".format(dim))\n",
    "    num_rows = data_array.shape[0]\n",
    "    if num_rows < 2:\n",
    "        raise ValueError(\"Argument data_array must have at least 2 rows, it actually has just {0}\".format(num_rows))\n",
    "    # Fill in with the correct float\n",
    "    num_training = int(0.75 * data_array.shape[0])\n",
    "    permuted_indices = np.random.permutation(data_array.shape[0])\n",
    "    return data_array[permuted_indices[:num_training], :], data_array[permuted_indices[num_training:], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current working directory in the IPython console is the tests folder that contains all tests. The test class TestSplitIntoTrainingAndTestingSets resides in the test module tests/models/test_train.py.\n",
    "\n",
    "What is the correct command to run all the tests in this test class using node IDs?\n",
    "- `!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets`\n",
    "What is the correct command to run only the previously failing test test_on_six_rows() using node IDs?\n",
    "- `!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets::test_on_six_rows`\n",
    "What is the correct command to run the tests in TestSplitIntoTrainingAndTestingSets using keyword expressions?\n",
    "- `!pytest -k \"SplitInto\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the tests in the test class TestModelTest in the IPython console. \n",
    "- `!pytest models/test_train.py::TestModelTest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mark the whole test class TestModelTest as \"expected to fail\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mark the whole test class as \"expected to fail\"\n",
    "# @pytest.mark.xfail\n",
    "# class TestModelTest(object):\n",
    "#     def test_on_linear_data(self):\n",
    "#         test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "#         expected = 1.0\n",
    "#         actual = model_test(test_input, 2.0, 1.0)\n",
    "#         message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n",
    "#         assert actual == pytest.approx(expected), message\n",
    "        \n",
    "#     def test_on_one_dimensional_array(self):\n",
    "#         test_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "#         with pytest.raises(ValueError) as exc_info:\n",
    "#             model_test(test_input, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add the following reason for the expected failure: \"Using TDD, model_test() has not yet been implemented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add a reason for the expected failure\n",
    "# @pytest.mark.xfail(reason=\"Using TDD, model_test() has not yet been implemented\")\n",
    "# class TestModelTest(object):\n",
    "#     def test_on_linear_data(self):\n",
    "#         test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "#         expected = 1.0\n",
    "#         actual = model_test(test_input, 2.0, 1.0)\n",
    "#         message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n",
    "#         assert actual == pytest.approx(expected), message\n",
    "        \n",
    "#     def test_on_one_dimensional_array(self):\n",
    "#         test_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "#         with pytest.raises(ValueError) as exc_info:\n",
    "#             model_test(test_input, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the tests in the test class TestGetDataAsNumpyArray in the IPython console.\n",
    "- `!pytest features/test_as_numpy.py::TestGetDataAsNumpyArray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the sys module.\n",
    "- Mark the test test_on_clean_file() as skipped if the Python version is greater than 2.7.\n",
    "- Add the following reason for skipping the test: \"Works only on Python 2.7 or lower\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the sys module\n",
    "# import sys\n",
    "\n",
    "# class TestGetDataAsNumpyArray(object):\n",
    "#     # Add a reason for skipping the test\n",
    "#     @pytest.mark.skipif(sys.version_info > (2, 7), reason=\"Works only on Python 2.7 or lower\")\n",
    "#     def test_on_clean_file(self):\n",
    "#         expected = np.array([[2081.0, 314942.0],\n",
    "#                              [1059.0, 186606.0],\n",
    "#                              [1148.0, 206186.0]\n",
    "#                              ]\n",
    "#                             )\n",
    "#         actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "#         message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "#         assert actual == pytest.approx(expected), message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the command that would only show the reason for expected failures in the test result report?\n",
    "- `!pytest -rx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the command that would only show the reason for skipped tests in the test result report?\n",
    "- `!pytest -rs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the command that would show the reason for both skipped tests and tests that are expected to fail in the test result report?\n",
    "- `!pytest -rsx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the GitHub repository of a Python package, you see the following badge: \"build failing\"\n",
    "- Since a build failing badge is indicative of bugs, the maintainer of any package should strive to keep this badge green (\"passing\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Github repository of a Python package, you see the following badge: \"codecov 85%\"\n",
    "- The test suite tests about 85% of the application code."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
